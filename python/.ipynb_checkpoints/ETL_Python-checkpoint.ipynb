{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "norman-static",
   "metadata": {},
   "source": [
    "# Data Challenge - loadsmart\n",
    "## By: Cesar Santos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "uniform-killer",
   "metadata": {},
   "source": [
    "Development Steps:\n",
    "* Importing data from Excel\n",
    "* Treathing data with Python (Pandas and Numpy)\n",
    "* Defining a star schema data model\n",
    "* Inserting treated data into a DW (MySQL localhost DB)\n",
    "* Creating a data visualization portifolio consuming data from the DW"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crazy-charlotte",
   "metadata": {},
   "source": [
    "### The diagram below shows my basic idea for the data modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "specialized-explanation",
   "metadata": {},
   "source": [
    "![MER Diagram](..\\documentation\\loadsmart_diagram-MER.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "catholic-hartford",
   "metadata": {},
   "source": [
    "## Required Libs: \n",
    "pandas=1.2.3\n",
    "\n",
    "openpyxl==3.0.7\n",
    "\n",
    "pymysql==0.7.2\n",
    "\n",
    "numpy==1.20.1\n",
    " \n",
    "sqlalchemy==1.4.3\n",
    "\n",
    "**Please use the requirements.txt file to intall theses required libs**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "killing-thanks",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "affected-exemption",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pymysql.cursors\n",
    "from sqlalchemy import create_engine"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "steady-communication",
   "metadata": {},
   "source": [
    "## Data Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-satin",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data Extraction\n",
    "source_df = pd.read_excel(r'..\\2017 Data- Challenge BI.xlsx', sheet_name='2017 Data',header=0, engine='openpyxl')\n",
    "\n",
    "#Removing specfic duplicated column\n",
    "source_df.drop(columns=['has_mobile_app_tracking.1'], inplace=True) \n",
    "\n",
    "source_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "incredible-remainder",
   "metadata": {},
   "source": [
    "## Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "reduced-restriction",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PYMYSQL AS OPTION\n",
    "connection = pymysql.connect(user = 'root', \n",
    "                       database = 'loadsmart_dw',                       \n",
    "                       host = 'localhost')\n",
    "\n",
    "#SQLALCHEMY AS OTHER OPTION\n",
    "engine = create_engine('mysql+pymysql://loadsmart_user:loadsmart_user@localhost/loadsmart_dw')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "endangered-scheme",
   "metadata": {},
   "source": [
    "## Dim Shipper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "retained-carol",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Dimension Shipper\n",
    "DimShipper_df = source_df[['shipper_name']].drop_duplicates()\n",
    "\n",
    "DimShipper_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "promotional-indianapolis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column list for insertion\n",
    "datacolumns = \", \".join([str('`'+i+'`') for i in DimShipper_df.columns.tolist()])\n",
    "\n",
    "for i,row in DimShipper_df.iterrows():\n",
    "    try:\n",
    "        sql = \"INSERT INTO `DimShipper` (\" +datacolumns + \", `last_update`) VALUES (\" + \"%s,\"*len(row) + \"'\"+ str(datetime.today())[:19]+ \"')\"\n",
    "\n",
    "        connection.cursor().execute(sql, tuple(row))\n",
    "        connection.commit()\n",
    "    except:\n",
    "        connection.commit()\n",
    "        print('Not Inserted:')\n",
    "        print(tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-personal",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Releasing memory\n",
    "del DimShipper_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "liquid-population",
   "metadata": {},
   "source": [
    "## Dim Route"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fundamental-theory",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension Route\n",
    "DimRoute_df = source_df[['lane']].drop_duplicates()\n",
    "\n",
    "DimRoute_df = DimRoute_df.join(DimRoute_df['lane'].str.split(' -> ', 1,expand=True).rename(columns={0:'source', 1:'destination'}))\n",
    "\n",
    "DimRoute_df = DimRoute_df.where(pd.notnull(DimRoute_df), None)\n",
    "\n",
    "DimRoute_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-andorra",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column list for insertion\n",
    "datacolumns = \", \".join([str('`'+i+'`') for i in DimRoute_df.columns.tolist()])\n",
    "\n",
    "for i,row in DimRoute_df.iterrows():\n",
    "    try:\n",
    "        sql = \"INSERT INTO `DimRoute` (\" +datacolumns + \", `last_update`) VALUES (\" + \"%s,\"*len(row) + \"'\"+ str(datetime.today())[:19]+ \"')\"\n",
    "\n",
    "        connection.cursor().execute(sql, tuple(row))\n",
    "        connection.commit()\n",
    "    except:\n",
    "        connection.commit()\n",
    "        print('Not Inserted:')\n",
    "        print(tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eight-point",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Releasing memory\n",
    "del DimRoute_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innocent-shock",
   "metadata": {},
   "source": [
    "## Dim Carrier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-characterization",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension Carrier\n",
    "DimCarrier_df = source_df[['carrier_name','carrier_rating','vip_carrier','carrier_dropped_us_count',\n",
    "                           'has_mobile_app_tracking','has_macropoint_tracking','has_edi_tracking']].drop_duplicates()\n",
    "\n",
    "# I didn't understood the column below, so i removed from my data modeling. With more specifc details I'd include this column inside fact table or dimesion.\n",
    "DimCarrier_df.drop(columns=['carrier_dropped_us_count'], inplace=True)\n",
    "\n",
    "# Removing duplicated recors after removing carrier_dropped_us_count\n",
    "DimCarrier_df = DimCarrier_df.drop_duplicates()\n",
    "\n",
    "DimCarrier_df.rename(columns={'vip_carrier':'fl_vip_carrier',\n",
    "                              'has_mobile_app_tracking':'fl_has_mobile_app_tracking',\n",
    "                              'has_macropoint_tracking':'fl_has_macropoint_tracking',\n",
    "                              'has_edi_tracking':'fl_has_edi_tracking'\n",
    "                             }, inplace=True)\n",
    "\n",
    "DimCarrier_df = DimCarrier_df.where(pd.notnull(DimCarrier_df), None)\n",
    "\n",
    "#Removing carrier nulls, will be replace by -1 in the fact table\n",
    "DimCarrier_df = DimCarrier_df.loc[DimCarrier_df['carrier_name']!= None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "killing-estonia",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column list for insertion\n",
    "datacolumns = \", \".join([str('`'+i+'`') for i in DimCarrier_df.columns.tolist()])\n",
    "\n",
    "for i,row in DimCarrier_df.iterrows():\n",
    "    \n",
    "    try:\n",
    "        sql = \"INSERT INTO `DimCarrier` (\" +datacolumns + \", `last_update`) VALUES (\" + \"%s,\"*len(row) + \"'\"+ str(datetime.today())[:19]+ \"')\"\n",
    "\n",
    "        connection.cursor().execute(sql, tuple(row))\n",
    "        connection.commit()\n",
    "    except:\n",
    "        connection.commit()\n",
    "        print('Not Inserted:')\n",
    "        print(tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-subcommittee",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Releasing memory\n",
    "del DimCarrier_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grand-enhancement",
   "metadata": {},
   "source": [
    "## Dim Load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "static-pioneer",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dimension Load\n",
    "DimLoad_df = source_df[['loadsmart_id','equipment_type','sourcing_channel','contracted_load',\n",
    "                        'load_booked_autonomously','load_sourced_autonomously','load_was_cancelled']].drop_duplicates()\n",
    "\n",
    "DimLoad_df.rename(columns={'contracted_load':'fl_contracted_load',\n",
    "                            'load_booked_autonomously':'fl_load_booked_autonomously',\n",
    "                            'load_sourced_autonomously':'fl_load_sourced_autonomously',\n",
    "                            'load_was_cancelled':'fl_load_was_cancelled'\n",
    "                             }, inplace=True)\n",
    "\n",
    "DimLoad_df = DimLoad_df.where(pd.notnull(DimLoad_df), None)\n",
    "\n",
    "DimLoad_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "romantic-password",
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating column list for insertion\n",
    "datacolumns = \", \".join([str('`'+i+'`') for i in DimLoad_df.columns.tolist()])\n",
    "\n",
    "for i,row in DimLoad_df.iterrows():\n",
    "    try:\n",
    "        sql = \"INSERT INTO `DimLoad` (\" +datacolumns + \", `last_update`) VALUES (\" + \"%s,\"*len(row) + \"'\"+ str(datetime.today())[:19]+ \"')\"\n",
    "            \n",
    "        connection.cursor().execute(sql, tuple(row))\n",
    "        connection.commit()\n",
    "    except:\n",
    "        connection.commit()\n",
    "        print('Not Inserted:')\n",
    "        print(tuple(row))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dried-flush",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Releasing memory\n",
    "del DimLoad_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "asian-prospect",
   "metadata": {},
   "source": [
    "## Load Dimensions On Memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painted-nurse",
   "metadata": {},
   "outputs": [],
   "source": [
    "DimShipper = pd.read_sql_query(\n",
    "'''select max(sk_code) as sk_code, shipper_name from DimShipper group by shipper_name''', connection)\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "DimShipper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "independent-mineral",
   "metadata": {},
   "outputs": [],
   "source": [
    "DimRoute = pd.read_sql_query(\n",
    "'''select max(sk_code) as sk_code, lane from DimRoute group by lane''', connection)\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "DimShipper.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "considered-custom",
   "metadata": {},
   "outputs": [],
   "source": [
    "DimCarrier = pd.read_sql_query(\n",
    "'''select max(sk_code) as sk_code, carrier_name from DimCarrier group by carrier_name''', connection)\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "DimCarrier.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "billion-scenario",
   "metadata": {},
   "outputs": [],
   "source": [
    "DimLoad = pd.read_sql_query(\n",
    "'''select max(sk_code) as sk_code, loadsmart_id from DimLoad group by loadsmart_id''', connection)\n",
    "\n",
    "connection.commit()\n",
    "\n",
    "DimLoad.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "listed-teaching",
   "metadata": {},
   "source": [
    "## Closing Database Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "boxed-metallic",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Closing pymysql connection\n",
    "connection.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cognitive-grave",
   "metadata": {},
   "source": [
    "## FactSales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fallen-spencer",
   "metadata": {},
   "outputs": [],
   "source": [
    "FactSales = source_df[['loadsmart_id','carrier_name','lane','shipper_name','quote_date',\n",
    "                       'book_date','source_date','book_price','source_price','pnl']]\n",
    "\n",
    "FactSales = FactSales.rename(columns={'quote_date':'quote_datetime',\n",
    "                         'book_date':'book_datetime',\n",
    "                         'source_date':'source_datetime',\n",
    "                         'pnl':'pnl_value'\n",
    "                        })\n",
    "\n",
    "#Creating minutes_between_quote_book metric\n",
    "FactSales['minutes_between_quote_book'] = FactSales['book_datetime'] - FactSales['quote_datetime'] \n",
    "FactSales['minutes_between_quote_book'] = FactSales['minutes_between_quote_book']/np.timedelta64(1,'m')\n",
    "FactSales['minutes_between_quote_book'] = FactSales['minutes_between_quote_book'].astype(int)\n",
    "\n",
    "#Replacing business key with surrogate key\n",
    "FactSales['sk_load_code'] = FactSales['loadsmart_id'].map(DimLoad.set_index('loadsmart_id')['sk_code'])\n",
    "FactSales['sk_load_code'] = FactSales['sk_load_code'].where(pd.notnull(FactSales['sk_load_code']), -1)\n",
    "\n",
    "FactSales['sk_carrier_code'] = FactSales['carrier_name'].map(DimCarrier.set_index('carrier_name')['sk_code'])\n",
    "FactSales['sk_carrier_code'] = FactSales['sk_carrier_code'].where(pd.notnull(FactSales['sk_carrier_code']), -1)\n",
    "\n",
    "FactSales['sk_route_code'] = FactSales['lane'].map(DimRoute.set_index('lane')['sk_code'])\n",
    "FactSales['sk_route_code'] = FactSales['sk_route_code'].where(pd.notnull(FactSales['sk_route_code']), -1)\n",
    "\n",
    "FactSales['sk_shipper_code'] = FactSales['shipper_name'].map(DimShipper.set_index('shipper_name')['sk_code'])\n",
    "FactSales['sk_shipper_code'] = FactSales['sk_shipper_code'].where(pd.notnull(FactSales['sk_shipper_code']), -1)\n",
    "                                                                                         \n",
    "\n",
    "FactSales = FactSales.where(pd.notnull(FactSales), None)\n",
    "\n",
    "#Replacing empty columns to avoid errors\n",
    "FactSales['source_datetime'] = FactSales['source_datetime'].astype(object).where(FactSales['source_datetime'].notnull(), None)\n",
    "\n",
    "#Removing business keys\n",
    "FactSales = FactSales.drop(columns=['loadsmart_id', 'carrier_name', 'lane', 'shipper_name'])\n",
    "\n",
    "reorder_columns = [\"sk_carrier_code\",\"sk_shipper_code\",\"sk_load_code\",\"sk_route_code\",\"quote_datetime\",\n",
    "                \"book_datetime\",\"source_datetime\",\"book_price\",\n",
    "                \"source_price\",\"pnl_value\",\"minutes_between_quote_book\"]\n",
    "#Reordering dataset\n",
    "FactSales = FactSales.reindex(columns=reorder_columns)\n",
    "\n",
    "FactSales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "european-footage",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as conn:\n",
    "    FactSales.to_sql('FactSales', con=conn, if_exists='append', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surrounded-tobago",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Releasing memory\n",
    "del FactSales"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rubber-election",
   "metadata": {},
   "source": [
    "## FactTransport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "original-george",
   "metadata": {},
   "outputs": [],
   "source": [
    "FactTransport = source_df[['loadsmart_id','carrier_name','lane','shipper_name','pickup_date',\n",
    "                           'pickup_appointment_time','carrier_on_time_to_pickup','delivery_date',\n",
    "                           'delivery_appointment_time','carrier_on_time_to_delivery','carrier_on_time_overall',\n",
    "                           'mileage']]\n",
    "\n",
    "FactTransport = FactTransport.rename(columns={'pickup_date':'pickup_datetime',\n",
    "                         'pickup_appointment_time':'pickup_appointment_datetime',\n",
    "                         'delivery_date':'delivery_datetime',\n",
    "                         'carrier_on_time_to_pickup':'fl_carrier_on_time_to_pickup',\n",
    "                         'delivery_appointment_time':'delivery_appointment_datetime',\n",
    "                         'carrier_on_time_to_delivery':'fl_carrier_on_time_to_delivery',\n",
    "                         'carrier_on_time_overall':'fl_carrier_on_time_overall',\n",
    "                         'mileage':'mileage_value'\n",
    "                        })\n",
    "\n",
    "#Creating days_between_pickup_delivery metric\n",
    "FactTransport['days_between_pickup_delivery'] = FactTransport['delivery_datetime'] - FactTransport['pickup_datetime'] \n",
    "FactTransport['days_between_pickup_delivery'] = FactTransport['days_between_pickup_delivery']/np.timedelta64(1,'D')\n",
    "FactTransport['days_between_pickup_delivery'] = FactTransport['days_between_pickup_delivery'].astype(int)\n",
    "\n",
    "#Creating hours_between_pickup_delivery metric\n",
    "FactTransport['hours_between_pickup_delivery'] = FactTransport['delivery_datetime'] - FactTransport['pickup_datetime'] \n",
    "FactTransport['hours_between_pickup_delivery'] = FactTransport['hours_between_pickup_delivery']/np.timedelta64(1,'h')\n",
    "FactTransport['hours_between_pickup_delivery'] = FactTransport['hours_between_pickup_delivery'].astype(int)\n",
    "\n",
    "#Replacing business key with surrogate key\n",
    "FactTransport['sk_load_code'] = FactTransport['loadsmart_id'].map(DimLoad.set_index('loadsmart_id')['sk_code'])\n",
    "FactTransport['sk_load_code'] = FactTransport['sk_load_code'].where(pd.notnull(FactTransport['sk_load_code']), -1)\n",
    "\n",
    "FactTransport['sk_carrier_code'] = FactTransport['carrier_name'].map(DimCarrier.set_index('carrier_name')['sk_code'])\n",
    "FactTransport['sk_carrier_code'] = FactTransport['sk_carrier_code'].where(pd.notnull(FactTransport['sk_carrier_code']), -1)\n",
    "\n",
    "FactTransport['sk_route_code'] = FactTransport['lane'].map(DimRoute.set_index('lane')['sk_code'])\n",
    "FactTransport['sk_route_code'] = FactTransport['sk_route_code'].where(pd.notnull(FactTransport['sk_route_code']), -1)\n",
    "\n",
    "FactTransport['sk_shipper_code'] = FactTransport['shipper_name'].map(DimShipper.set_index('shipper_name')['sk_code'])\n",
    "FactTransport['sk_shipper_code'] = FactTransport['sk_shipper_code'].where(pd.notnull(FactTransport['sk_shipper_code']), -1)\n",
    "\n",
    "FactTransport = FactTransport.where(pd.notnull(FactTransport), None)\n",
    "\n",
    "#Removing business keys\n",
    "FactTransport = FactTransport.drop(columns=['loadsmart_id', 'carrier_name', 'lane', 'shipper_name'])\n",
    "\n",
    "reorder_columns = [\"sk_carrier_code\",\"sk_shipper_code\",\"sk_load_code\",\"sk_route_code\",\"pickup_datetime\",\n",
    "                \"pickup_appointment_datetime\",\"fl_carrier_on_time_to_pickup\",\"delivery_datetime\",\n",
    "                \"delivery_appointment_datetime\",\"fl_carrier_on_time_to_delivery\",\"fl_carrier_on_time_overall\",\n",
    "                \"mileage_value\",\"days_between_pickup_delivery\",\"hours_between_pickup_delivery\"]\n",
    "\n",
    "#Reordering dataset\n",
    "FactTransport = FactTransport.reindex(columns=reorder_columns)\n",
    "\n",
    "FactTransport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mediterranean-initial",
   "metadata": {},
   "outputs": [],
   "source": [
    "with engine.begin() as conn:\n",
    "    FactTransport.to_sql('FactTransport', con=conn, if_exists='append', index= False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "occasional-seventh",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Releasing memory\n",
    "del FactTransport"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "innovative-sound",
   "metadata": {},
   "source": [
    "## Cleaning memory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intended-alberta",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Releasing DimLoad\n",
    "del DimLoad\n",
    "\n",
    "#Releasing DimShipper\n",
    "del DimShipper\n",
    "\n",
    "#Releasing DimCarrier\n",
    "del DimCarrier\n",
    "\n",
    "#Releasing DimRoute\n",
    "del DimRoute"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
